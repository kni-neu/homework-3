\documentclass[paper=a4, fontsize=11pt]{scrartcl} % A4 paper and 11pt font size

\newcommand{\assignment}{3}
\newcommand{\duedate}{February 1, 2023}
\input{hw-template.tex}

\begin{document}
\maketitle % Print the title

%%%%%%%%%%%%%%%%%%%%
\section{Preprocessing Raw Data from Multiple Sources}
%%%%%%%%%%%%%%%%%%%%

The purpose of this section is to build training pre-processing code that would enable us to do machine learning. Typically there are multiple data sources, and in order to create training samples, we'll need to join several of them together. Also, in subsequent lectures, we'll learn that several NLP algorithms rely on self-supervision (using the existing data in the absence of labels) in order to train large scale neural networks. In this assignment we'll explore the process of getting multiple raw data sources into the form of training data, a common task that engineers spend a significant amount of time doing. 

\subsection{The Dataset}

News articles are commonly aggregated from multiple sources. The landscape of news has been evolving ever since social media has amplified its effects, and a big topic that has come up in Congress is the idea of bias. That is, news articles may cover news stories with differing types of verbiage and language at varying frequencies. 

The data that we will be using today comes from Kaggle, and it is available \href{https://course.ccs.neu.edu/cs6220/homework-3/}{here}. There are two CSV files that we wish to join this week's homework:

\begin{itemize}
    \item \verb"data/id_titles.csv"
    \item \verb"data/id_publishers.csv"
\end{itemize}

As there name suggests, these are publishers and titles paired with data, along with a lot of other data. Each record as an ID associated with it and it is possible to join the two data sources.

\subsection{The Tensorflow Example}
One of the most common ways to pass messages from one place to another in real-time is through protocol buffers, \emph{protobufs} for short. Protobufs are language-neutral and developed by Google to serialize data (convert into bytes), efficiently stored (in binary form), flexibly be defined, and subsequently loaded into memory quickly. 

Tensorflow Records (TFRecords) or TFExamples are a protobuf that has been the standard for many machine learning serving systems, and take up significantly less space than the original data on distributed file systems, often partitioned into multiple files (i.e., \verb"SSTables" through \verb"BigTable"s.) They can be read in with parallel I/O, which can be useful for training machine learning algorithms on large scale data.

If you look at Tensorflow Example's proto structure~\cite{tfproto}, they use the \verb"Feature" proto~\cite{tffeat}:

\begin{verbatim}
import "tensorflow/core/example/feature.proto";

message Example {
  Features features = 1;
}
\end{verbatim}

Each feature can be of type \verb"bytes", \verb"float", or \verb"int64". As well, you can have arrays of variable length of the above data types (that's what meant by the \verb"repeated" in the \verb"*.proto" message. Please review Tensorflow Example's proto, and there are good resources on how to read and write examples in Python~\cite{tfcodex}. At a high level, the following is an example of how Tensorflow Examples are structured:

\begin{verbatim}
example = text_format.Parse('''
   features {
       feature {
         key: "my_feature"
         value {int64_list {value: [1, 2, 3, 4]}}
       }
       feature {
         key: "string_array"
         value {bytes_list {value: ["a", "b", "c", "d"]}}
       }
   }''', tf.train.Example())
\end{verbatim}

(Please use more standard data ingestion rather than creating a string in protobuf format.)

\subsection{Data Exploration}

In most data exploration, we would like to take the distribution of the data. In this case, we can use a histogram of the words. Oftentimes, scientists will cut off extremely frequent words and extremely infrequent words.

In this subsection, take the distribution of the words in all of the titles in the dataset. Plot out the distribution, and provide some design decisions as to what words you're deciding to include in the distribution.

\subsection{Create Training Data}

Word2Vec~\cite{word2vec} is a popular self-supervised algorithm that windows surrounding words in order create analytics, a technique called the skip-gram. That is to say, the training set consists of 

\begin{itemize}
    \item An anchor word
    \item Its surrounding words in an array [word1, word2, word3, ...]. These surrounding words are centered around the anchor word.
\end{itemize}

So, for example, let's say we had the following sentence

\vspace*{\fill} 
\begin{quote} 
\centering 
The rain in Spain falls mainly on the plains.
\end{quote}
\vspace*{\fill}

After preprocessing the sentence, for a window of size $5$, the corresponding \verb"tf.Example"s for the anchor \verb"Spain" would be:

\begin{verbatim}
example = text_format.Parse('''
   features {
       feature {
         key: "anchor_word"
         value {bytes_list: "spain"}
       }
       feature {
         key: "surrounding_words"
         value {bytes_list {value: ["rain", "in", "falls", "mainly"]}}
       }
       feature {
         key: "news_source"
         value {bytes_list: "<news_source>"}
       }
       feature {
         key: "time"
         value {int64_list: 204}
       }
   }''', tf.train.Example())
\end{verbatim}

Because the window is five, we pick two words on either side of \verb"Spain" (which we have lowercased as \verb"spain"). On the left side, these words are \verb"rain" and \verb"in". On the right side, these words are \verb"falls" and \verb"mainly". Therefore the feature array would be \{\verb"rain", \verb"in", \verb"falls", \verb"mainly"\}.

Very often you will find that the window will come up against the edge of text. Only include words in the window, and if the window surpass the end or beginning of the title, you need not include anything. For example, the anchor \verb"plains" has no words on the right side of the anchor. Therefore, the feature array would be \{\verb"on", \verb"the"\}, an array of size two. Likewise, the anchor \verb"rain" will have only a single word \verb"the" to the left side of the anchor in the window. Therefore, the feature array would be \{\verb"the", \verb"in", \verb"spain"\}, an array of size three.

Notice there are other features. Those will become clearer in the next section.

\subsection{Adding in News Source and Time}

You'll notice two sources of data. You'll need to join the two to understand who wrote what article. The common field between the two is ID. Explain what type of \verb"join" you used and why. Provide examples and samples of data as supporting evidence.

With the joined data, add in the feature of who wrote the story. It is at your discretion which field to use for this data (i.e., 
 \verb"PUBLISHER", \verb"URL", or \verb"HOSTNAME"). Be able to justify your design decision. Provide some analysis (histograms, etc.). 

 Finally, add in time information. A common format for timing in computers is UTM time~\cite{timestamp}, which is the provided \verb"TIMESTAMP". This is often not useful as a feature for machine learning algorithms. Think back to a business use case of how we'd like to use our data. For example, would we want to know about trending words (e.g., ``millenial'', ``covid'', etc.) or countries (e.g., ``korea'', ``taiwan'', ``mexico'', etc.) on an hourly basis? A monthly basis? There is no correct answer, so please justify your design decision. (Sometimes it helps to understand the trends in the data.)


\section{Submission Instructions}

\begin{itemize}
    \item Commit your code to main in Github, and provide the link. We will snapshot and download it. If you created your code in Colab, you can download your Colab as an iPython Notebook by going to \verb"Download" and selecting \verb"Download .ipynb". 
    \item Include 10,000 samples in your repository and call it \verb"output.tfrecord".
    \item Your documentation and code's legibility is part of our grading criterion, so please make sure it's readable.
    \item In your writeup (i.e., \verb"assignment3.pdf"), include a pipeline description with a diagram.
\end{itemize}




\begin{thebibliography}{2}
\bibitem{tfproto} Tensorflow Example Protobuf: \url{https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/example/example.proto}
\bibitem{tfcodex} Example Reading and Writing Examples in Tensorflow \url{https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/load_data/tfrecord.ipynb} 
\bibitem{tffeat} Tensorflow Feature Protobuf:
\url{https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/example/feature.proto}
\bibitem{protos} Google's Docs on Protocol Buffers:
\url{https://developers.google.com/protocol-buffers}
\bibitem{word2vec} Mikolov, Tomas; et al. (2013). "Efficient Estimation of Word Representations in Vector Space". arXiv:1301.3781
\bibitem{timestamp} The UNIX Timestamp \url{https://en.wikipedia.org/wiki/Unix_time}
\end{thebibliography}

\end{document}
